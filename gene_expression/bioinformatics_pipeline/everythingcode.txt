## Tag-based RNA-seq (Tag-Seq) reads processing pipeline, version November 9, 2023
# Created by Misha Matz (matz@utexas.edu), modified by Michael Studivan (studivanms@gmail.com) for use on the FAU KoKo HPC
# then modified by Allyson DeMerlis for use on the University of Miami HPC "Pegasus"

# put all raw reads into one directory: 1_fastq_rawreads

# they are all in .fastq.gz file format. First need to look at the reads and ensure that there are 4 lines per read (look for sequence headers)

gunzip SampleName.fastq.gz

# look at the reads
head -50 SampleName.fastq
# note that every read has four lines, the ID line starts with @HWI

#------------------------------

# first step: count the raw reads of each sample (converted countreads.pl to a bash script)

#!/bin/bash
#BSUB -J countrawreads
#BSUB -q general
#BSUB -P and_transcriptomics
#BSUB -n 8
#BSUB -W 120:00
#BSUB -o countrawreads.out
#BSUB -e countrawreads.err
#BSUB -u and128@miami.edu
#BSUB -N

#Purpose: counts the number of Illumina reads in a bunch of fastq files

#specify variables and paths

and="/scratch/projects/and_transcriptomics"

cd "/scratch/projects/and_transcriptomics/Ch2_temperaturevariability2023/1_fastq_rawreads"

output_file="countreads_results.txt"

glob=".fastq"
if [ ! -z "$1" ]; then
    glob="$1"
fi

fqs=(*$glob)
for f in "${fqs[@]}"; do
    gunzip -c "$f" > "temp.fastq"  # Decompress the file to a temporary file
    nrd=$(cat "temp.fastq" | wc -l)
    nrd=$((nrd / 4))
    echo "$f	$nrd"
    echo "$f	$nrd" >> "$output_file"  # Append the results to the output file
    rm "temp.fastq"  # Remove the temporary file
done

echo "Results have been saved to $output_file"

#------------------------------

# For the next step, I will be using TrimGalore to cutadapt.

# First, run these lines (if you haven't done this before) to install the conda packages (including cutadapt) so all the environments will work:

# module load
# conda config --add channels defaults
# conda config --add channels bioconda
# conda config --add channels conda-forge

# create cutadapt conda environment

conda create -n cutadaptenv cutadapt

# Modifications to Michael's code:
# 1. for Pegasus, you can't run "conda activate" within a LSF job, you have to active the conda environment first and then submit the job in that conda environment.
# 2. you can download all the .pl scripts from the tagseq github repository, but then you need to make them executable (run chmod +x scriptname.pl)
# 3. Also, you need to edit the header of each .pl script so it says "#!/usr/bin/env perl"

#------------------------------
## Removing adaptors and low quality reads

# module load anaconda

source /share/apps/anaconda/anaconda3_build/bin/activate

# activate cutadapt environment

conda activate cutadaptenv

#! /usr/bin/env bash

#define variables for directories and files
and="/scratch/projects/and_transcriptomics"
project="and_transcriptomics"
projdir="/scratch/projects/and_transcriptomics/Ch2_temperaturevariability2023/2_trimmed_reads"

cd "/scratch/projects/and_transcriptomics/Ch2_temperaturevariability2023/1_fastq_rawreads"

data=($(ls *.fastq.gz))

for samp in "${data[@]}" ; do \

#build script
echo "making cutadapt script for ${samp}..."
echo "
#! /usr/bin/env bash
#BSUB -P ${project}
#BSUB -J ${samp}_trim
#BSUB -e ${projdir}/logs/${samp}_trim.err
#BSUB -o ${projdir}/logs/${samp}_trim.out
#BSUB -W 12:00
#BSUB -n 8
#BSUB -q general

cd \"/scratch/projects/and_transcriptomics/Ch2_temperaturevariability2023/1_fastq_rawreads\"

echo \"Gunzipping file  ${samp} then removing adaptors and low quality reads...\"

gunzip -c ${samp} > ${samp}_temp.fastq
${and}/Ch2_temperaturevariability2023/0_scripts/tagseq_clipper.pl ${samp}_temp.fastq | cutadapt - -a AAAAAAAA -a AGATCGG -q 15 -m 25 -o ${projdir}/${samp/.fastq.gz/}.trim

" > ${projdir}/${samp}_trim.job

#submit script

bsub < ${projdir}/${samp}_trim.job

done

# extract number of good reads, headers, and duplicates from each sample job error file

for file in /scratch/projects/and_transcriptomics/Ch2_temperaturevariability2023/2_trimmed_reads/take_4/logs/*.err; do
    tail -n 1 "$file"
done > all_trimmed_read_counts.txt

# did the trimming work?
head -100 SampleName.fastq | grep -E '^[NACGT]+$'
head -100 SampleName.trim | grep -E '^[NACGT]+$'
# the long runs of base A should be gone

# to count the number of reads in trimmed samples
echo "countreads_trim.pl > countreads_trim.txt" > count_trim
launcher_creator.py -j count_trim -n count_trim -q shortq7 -t 6:00:00 -e studivanms@gmail.com
sbatch count_trim.slurm

#------------------------------
